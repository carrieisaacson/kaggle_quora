{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying text cleaning code from [KevinLiao159 (GitHub)](https://github.com/KevinLiao159/Quora/blob/master/kernels/submission_v50.py), who in turn was modifying code from [Heng Zheng (Kaggle)](https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694), who was actually using code written by [Theo Viel (Kaggle)](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2). Love ya Kaggle, don't ever change.\n",
    "\n",
    "Also very informative is [this kernel by Dieter (Kaggle)](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) which discusses how to customize preprocessing to the embedding chosen.\n",
    "\n",
    "I chose to work with Fasttext to start out, because this is the most recent text embedding available and thus included Trump and other more topical US political content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load test and train data from csv.\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    df_train: DataFrame\n",
    "        Full raw training dataset\n",
    "    df_test: DataFrame\n",
    "        Full raw test dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select local path vs kaggle kernel\n",
    "    path = os.getcwd()\n",
    "    if 'data-projects/kaggle_quora/notebooks' in path:\n",
    "        data_dir = '../data/raw/'\n",
    "    else:\n",
    "        data_dir = ''\n",
    "\n",
    "    df_train = pd.read_csv(data_dir +'train.csv')\n",
    "    df_test = pd.read_csv(data_dir +'test.csv')\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misspellings, misspacings, contractions and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are like children, if they don't know a word yet then they can't understand what you are trying to say. But we can correct to word to one they do recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw, df_test_raw = load_data()\n",
    "\n",
    "df_train = df_train_raw\n",
    "df_test = df_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embedding(filepath):\n",
    "    \"\"\"\n",
    "    given a filepath to embeddings file, return a word to vec\n",
    "    dictionary, in other words, word_embedding\n",
    "    E.g. {'word': array([0.1, 0.2, ...])}\n",
    "    \"\"\"\n",
    "    def _get_vec(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    print('load word embedding ......')\n",
    "    try:\n",
    "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(filepath))\n",
    "    except UnicodeDecodeError:\n",
    "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(\n",
    "            filepath, encoding=\"utf8\", errors='ignore'))\n",
    "\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word embedding ......\n"
     ]
    }
   ],
   "source": [
    "fasttext = load_word_embedding('../data/raw/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text / Document Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs):\n",
    "    \"\"\"\n",
    "    given a list or np.array of strings create a dictionary of unique words with frequencies.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    docs: list or np.array\n",
    "        iterable of text\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    dict\n",
    "        unique words as keys, frequencies as values\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "                \n",
    "    return vocab\n",
    "\n",
    "def vocab_embedding_coverage(vocab, embedding, verbose = False):\n",
    "    \"\"\"\n",
    "    given a dict representing the word frequency of a corpus, \n",
    "    calculate the percentage of unique words and \n",
    "    the percentage of the corpus matched in the embedding dict.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    vocab: dict\n",
    "        word frequency of corpus\n",
    "    embedding: dict\n",
    "        embedding vector converted to dict\n",
    "    verbse: bool\n",
    "        print summary statistics\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    \n",
    "    perc_words : float\n",
    "        percentage of unique words identified in corpus\n",
    "    perc_corpus : float\n",
    "        percentage of corpus identified in corpus\n",
    "    words_in_embedding: dict\n",
    "        dictionary of unique words, frequency and whether found in embedding (true / false)\n",
    "    \"\"\"\n",
    "    \n",
    "    words_in_embedding = {}\n",
    "    word_found_count = 0\n",
    "    corpus_found_count = 0\n",
    "    corpus_count = 0 \n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        corpus_count += freq\n",
    "        words_in_embedding[word] = {\n",
    "            'frequency': freq,\n",
    "            'embedding': (word in embedding)\n",
    "        }\n",
    "        if word in embedding:\n",
    "            word_found_count += 1\n",
    "            corpus_found_count += freq\n",
    "    \n",
    "    perc_words = word_found_count / len(vocab)\n",
    "    perc_corpus = corpus_found_count / corpus_count\n",
    "    \n",
    "    print('{}% of vocabulary words found in embedding files'.format(round(100*perc_words,2)))\n",
    "    print('{}% of corpus found in embedding files'.format(round(100*perc_corpus,2)))\n",
    "    \n",
    "    return perc_words, perc_corpus, words_in_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.77% of vocabulary words found in embedding files\n",
      "87.66% of corpus found in embedding files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>India?</th>\n",
       "      <td>17082</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don't</th>\n",
       "      <td>15642</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it?</th>\n",
       "      <td>13436</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'm</th>\n",
       "      <td>13344</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What's</th>\n",
       "      <td>12985</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do?</th>\n",
       "      <td>9112</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life?</th>\n",
       "      <td>8074</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can't</th>\n",
       "      <td>7375</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you?</th>\n",
       "      <td>6553</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me?</th>\n",
       "      <td>6485</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency  embedding\n",
       "India?  17082      False    \n",
       "don't   15642      False    \n",
       "it?     13436      False    \n",
       "I'm     13344      False    \n",
       "What's  12985      False    \n",
       "do?     9112       False    \n",
       "life?   8074       False    \n",
       "can't   7375       False    \n",
       "you?    6553       False    \n",
       "me?     6485       False    "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(np.concatenate((df_train.question_text, df_test.question_text)))\n",
    "w,c,words_in_embedding = vocab_embedding_coverage(vocab, fasttext, True)\n",
    "df_words = pd.DataFrame.from_dict(words_in_embedding, orient = 'index')\n",
    "df_words = df_words.sort_values(by='frequency', ascending=False)\n",
    "df_words[np.logical_not(df_words.embedding)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext does include punctuation. Notable that there are variants on terms like Ph.D and PhD, or ie., i.e. and i.e. \n",
    "Also interesting that the embedding contains references to websites like Google and Amazon. This seems useful.\n",
    "\n",
    "Stripping all punctuation shouldn't be strictly necessary. in the example of .NET or ASP.NET for example, I'd prefer to keep that period in so we can match. Hyphens may also be useulf, rather than figuring out whether to contract or space split the word.\n",
    "\n",
    "Fixing contractions is also going to be useful. After that apostrophe's can be removed.\n",
    "\n",
    "The presence of multiple or absence of question marks is associated with insincerity. The question mark is part of the embedding, so will space it so it.\n",
    "\n",
    "[punc for punc in fasttext.keys() if '.' in punc]\n",
    "\n",
    "['.',\n",
    " '...',\n",
    " 'U.S.',\n",
    " 'i.e.',  'ie.', 'i.e',\n",
    "'index.php',\n",
    " 'St.',\n",
    " 'en.wikipedia.org',\n",
    " 'p.m.',\n",
    " 'P.M.',\n",
    " 'D.C.',\n",
    " 'a.m.',\n",
    " 'Ph.D.',\n",
    " 'Ph.D',\n",
    " 'U.K.',\n",
    " 'www.youtube.com',\n",
    " 'Amazon.com',\n",
    " 'U.S.A.',\n",
    " 'N.Y.',\n",
    " '.jpg',\n",
    " 'index.html',\n",
    " '.NET',\n",
    " 'NYTimes.com',\n",
    " 'www.facebook.com',\n",
    " '6.6',\n",
    " '8.2',\n",
    " '4.9',\n",
    " 'Wikipedia-logo.png',\n",
    " '1.jpg',\n",
    " 'www.google.com',\n",
    " 'govt.',\n",
    " 'gmail.com',\n",
    " 'hotmail.com',\n",
    " 'twitter.com',\n",
    " 'CNN.com',\n",
    " 'S.H.I.E.L.D.',\n",
    " 'ASP.NET',\n",
    "  '.Net',\n",
    " 'Thanks.',\n",
    " 'Salesforce.com',\n",
    " 'msnbc.com',\n",
    " 'FoxNews.com.',\n",
    " 'www.nytimes.com',\n",
    " 'M.I.T.',\n",
    " 'Amazon.com.',\n",
    " 'Last.fm',\n",
    " ...]\n",
    " \n",
    " \n",
    "[punc for punc in fasttext.keys() if '?' in punc]\n",
    "\n",
    "['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    unicode string normalization\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\n",
    "def remove_newline(text):\n",
    "    \"\"\"\n",
    "    remove \\n and  \\t\n",
    "    \"\"\"\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\t', ' ', text)\n",
    "    text = re.sub('\\b', ' ', text)\n",
    "    text = re.sub('\\r', ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_latex(text):\n",
    "    \"\"\"\n",
    "    convert r\"[math]\\vec{x} + \\vec{y}\" to English\n",
    "    \"\"\"\n",
    "    # edge case\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
    "\n",
    "    pattern_to_sub = {\n",
    "        r'\\\\mathrm': ' LaTex math mode ',\n",
    "        r'\\\\mathbb': ' LaTex math mode ',\n",
    "        r'\\\\boxed': ' LaTex equation ',\n",
    "        r'\\\\begin': ' LaTex equation ',\n",
    "        r'\\\\end': ' LaTex equation ',\n",
    "        r'\\\\left': ' LaTex equation ',\n",
    "        r'\\\\right': ' LaTex equation ',\n",
    "        r'\\\\(over|under)brace': ' LaTex equation ',\n",
    "        r'\\\\text': ' LaTex equation ',\n",
    "        r'\\\\vec': ' vector ',\n",
    "        r'\\\\var': ' variable ',\n",
    "        r'\\\\theta': ' theta ',\n",
    "        r'\\\\mu': ' average ',\n",
    "        r'\\\\min': ' minimum ',\n",
    "        r'\\\\max': ' maximum ',\n",
    "        r'\\\\sum': ' + ',\n",
    "        r'\\\\times': ' * ',\n",
    "        r'\\\\cdot': ' * ',\n",
    "        r'\\\\hat': ' ^ ',\n",
    "        r'\\\\frac': ' / ',\n",
    "        r'\\\\div': ' / ',\n",
    "        r'\\\\sin': ' Sine ',\n",
    "        r'\\\\cos': ' Cosine ',\n",
    "        r'\\\\tan': ' Tangent ',\n",
    "        r'\\\\infty': ' infinity ',\n",
    "        r'\\\\int': ' integer ',\n",
    "        r'\\\\in': ' in ',\n",
    "    }\n",
    "    # post process for look up\n",
    "    pattern_dict = {k.strip('\\\\'): v for k, v in pattern_to_sub.items()}\n",
    "    # init re\n",
    "    patterns = pattern_to_sub.keys()\n",
    "    pattern_re = re.compile('(%s)' % '|'.join(patterns))\n",
    "\n",
    "    def _replace(match):\n",
    "        \"\"\"\n",
    "        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n",
    "        \"\"\"\n",
    "        return pattern_dict.get(match.group(0).strip('\\\\'), match.group(0))\n",
    "    return pattern_re.sub(_replace, text)\n",
    "\n",
    "def decontracted(text):\n",
    "    \"\"\"\n",
    "    author: Kevin Liao\n",
    "    \n",
    "    de-contract the contraction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # specific\n",
    "        text = re.sub(r\"(W|w)on(\\'|\\’)t\", \"will not\", text)\n",
    "        text = re.sub(r\"(C|c)an(\\'|\\’)t\", \"can not\", text)\n",
    "        text = re.sub(r\"(Y|y)(\\'|\\’)all\", \"you all\", text)\n",
    "        text = re.sub(r\"(Y|y)a(\\'|\\’)ll\", \"you all\", text)\n",
    "\n",
    "        # general\n",
    "        text = re.sub(r\"(I|i)(\\'|\\’)m\", \"i am\", text)\n",
    "        text = re.sub(r\"(A|a)in(\\'|\\’)t\", \"is not\", text)\n",
    "        text = re.sub(r\"n(\\'|\\’)t\", \" not\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)re\", \" are\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)s\", \" is\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)d\", \" would\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)ll\", \" will\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)t(?!h)\", \" not\", text)\n",
    "        text = re.sub(r\"(\\'|\\’)ve\", \" have\", text)\n",
    "    except:\n",
    "        print('error processing text:{}'.format(text))\n",
    "        \n",
    "    return text\n",
    "\n",
    "def remove_string(text, string_to_omit=['']):\n",
    "    \"\"\"\n",
    "    Substrings to delete if present.\n",
    "    \"\"\"    \n",
    "    # light arg checking\n",
    "    if type(string_to_omit) == str:\n",
    "        string_to_omit = [string_to_omit]\n",
    "    \n",
    "    re_tok = re.compile(f'({string_to_omit})')\n",
    "    return re_tok.sub(r'', text)    \n",
    "\n",
    "def spacing_digit(text):\n",
    "    \"\"\"\n",
    "    add space before and after digits\n",
    "    \"\"\"\n",
    "    re_tok = re.compile('([0-9])')\n",
    "    return re_tok.sub(r' \\1 ', text)\n",
    "\n",
    "\n",
    "def spacing_number(text):\n",
    "    \"\"\"\n",
    "    add space before and after numbers\n",
    "    \"\"\"\n",
    "    re_tok = re.compile('([0-9]{1,})')\n",
    "    return re_tok.sub(r' \\1 ', text)\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    \"\"\"\n",
    "    numbers are not toxic\n",
    "    \"\"\"\n",
    "    return re.sub('\\d+', ' ', text)\n",
    "\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('\\s+$', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_misspell(text):\n",
    "    \"\"\"\n",
    "    misspell list (quora vs. fasttext wiki-news-300d-1M)\n",
    "    \"\"\"\n",
    "    misspell_to_sub = {\n",
    "        '“': ' \" ',\n",
    "        '”': ' \" ',\n",
    "        '°C': 'degrees Celsius',\n",
    "        '&amp;': ' & ',\n",
    "        '2k17': '2017',\n",
    "        '2k18': '2018',\n",
    "        '9/11': 'terrorist attack',\n",
    "        'Aadhar': 'Indian identification number',\n",
    "        'aadhar': 'Indian identification number',\n",
    "        ' adhar': 'Indian identification number',\n",
    "        'Adityanath': 'Indian monk Yogi Adityanath',\n",
    "        'AFCAT': 'Indian air force recruitment exam',\n",
    "        'airhostess': 'air hostess',\n",
    "        'Ambedkarite': 'Dalit Buddhist movement ',\n",
    "        'AMCAT': 'Indian employment assessment examination',\n",
    "        'and/or': 'and or',\n",
    "        'antibrahmin': 'anti Brahminism',\n",
    "        'articleship': 'chartered accountant internship',\n",
    "        'Asifa': 'abduction rape murder case ',\n",
    "        'AT&T': 'telecommunication company',\n",
    "        'atrracted': 'attract',\n",
    "        'Awadesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Awdhesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Babchenko': 'Arkady Arkadyevich Babchenko faked death',\n",
    "        'Barracoon': 'Black slave',\n",
    "        'Bathla': 'Namit Bathla',\n",
    "        'bcom': 'bachelor of commerce',\n",
    "        'beyon´çe': 'Beyoncé',\n",
    "        'Bhakts': 'Bhakt',\n",
    "        'bhakts': 'Bhakt',\n",
    "        'bigdata': 'big data',\n",
    "        'biharis': 'Biharis',\n",
    "        'BIMARU': 'Bihar Madhya Pradesh Rajasthan Uttar Pradesh',\n",
    "        'BITSAT': 'Birla Institute of Technology entrance examination',\n",
    "        'BNBR': 'be nice be respectful',\n",
    "        'bodycams': 'body cams',\n",
    "        'bodyshame': 'body shaming',\n",
    "        'bodyshoppers': 'body shopping',\n",
    "        'Bolsonaro': 'Jair Bolsonaro',\n",
    "        'Boshniak': 'Bosniaks ',\n",
    "        'Boshniaks': 'Bosniaks',\n",
    "        'bremainer': 'anti Brexit',\n",
    "        'bremoaner': 'Brexit remainer',\n",
    "        'Brexiteer': 'Brexit supporter',\n",
    "        'Brexiteers': 'Brexit supporters',\n",
    "        'Brexiter': 'Brexit supporter',\n",
    "        'Brexiters': 'Brexit supporters',\n",
    "        'brexiters': 'Brexit supporters',\n",
    "        'Brexiting': 'Brexit',\n",
    "        'Brexitosis': 'Brexit disorder',\n",
    "        'Brexshit': 'Brexit bullshit',\n",
    "        'C#': 'computer programming language',\n",
    "        'c#': 'computer programming language',\n",
    "        'C++': 'computer programming language',\n",
    "        'c++': 'computer programming language',\n",
    "        'Cananybody': 'Can any body',\n",
    "        'cancelled': 'canceled',\n",
    "        'Castrater': 'castration',\n",
    "        'castrater': 'castration',\n",
    "        'centre': 'center',\n",
    "        'Chodu': 'fucker',\n",
    "        'Chutiya': 'Tibet people ',\n",
    "        'Chutiyas': 'Tibet people ',\n",
    "        'cishet': 'cisgender and heterosexual person',\n",
    "        'citicise': 'criticize',\n",
    "        'cliché': 'cliche',\n",
    "        'clichéd': 'cliche',\n",
    "        'clichés': 'cliche',\n",
    "        'Clickbait': 'click bait ',\n",
    "        'clickbait': 'click bait ',\n",
    "        'coinbase': 'bitcoin wallet',\n",
    "        'Coinbase': 'bitcoin wallet',\n",
    "        'colour': 'color',\n",
    "        'COMEDK': 'medical engineering and dental colleges of Karnataka entrance examination',\n",
    "        'counselling': 'counseling',\n",
    "        'Crimean': 'Crimea people ',\n",
    "        'currancies': 'currencies',\n",
    "        'currancy': 'currency',\n",
    "        'cybertrolling': 'cyber trolling',\n",
    "        'D&D': 'dungeons & dragons game',\n",
    "        'daesh': 'Islamic State of Iraq and the Levant',\n",
    "        'deadbody': 'dead body',\n",
    "        'deaddict': 'de addict',\n",
    "        'demcoratic': 'Democratic',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'Demonetization': 'demonetization',\n",
    "        'demonitisation': 'demonetization',\n",
    "        'demonitization': 'demonetization',\n",
    "        'deplorables': 'deplorable',\n",
    "        'doI': 'do I',\n",
    "        'Doklam': 'disputed Indian Chinese border area',\n",
    "        'Doklam': 'Tibet',\n",
    "        'Dönmeh': 'Islam',\n",
    "        'Dravidanadu': 'Dravida Nadu',\n",
    "        'dropshipping': 'drop shipping',\n",
    "        'Drumpf ': 'Donald Trump fool ',\n",
    "        'Drumpfs': 'Donald Trump fools',\n",
    "        'Dumbassistan': 'dumb ass Pakistan',\n",
    "        'emiratis': 'Emiratis',\n",
    "        'Eroupian': 'European',\n",
    "        'Etherium': 'Ethereum',\n",
    "        'Eurocentric': 'Eurocentrism ',\n",
    "        'exboyfriend': 'ex boyfriend',\n",
    "        'facetards': 'Facebook retards',\n",
    "        'Fadnavis': 'Indian politician Devendra Fadnavis',\n",
    "        'favourite': 'favorite',\n",
    "        'Fck': 'Fuck',\n",
    "        'fck': 'fuck',\n",
    "        'Feku': 'The Man of India ',\n",
    "        'feminazism': 'feminism nazi',\n",
    "        'FIITJEE': 'Indian tutoring service',\n",
    "        'fiitjee': 'Indian tutoring service',\n",
    "        'fortnite': 'Fortnite ',\n",
    "        'Fortnite': 'video game',\n",
    "        'Gixxer': 'motorcycle',\n",
    "        'Golang': 'computer programming language',\n",
    "        'golang': 'computer programming language',\n",
    "        'Gujratis': 'Gujarati',\n",
    "        'Gurmehar': 'Gurmehar Kaur Indian student activist',\n",
    "        'h1b': 'US work visa',\n",
    "        'H1B': 'US work visa',\n",
    "        'hairfall': 'hair loss',\n",
    "        'harrase': 'harass',\n",
    "        'he/she': 'he or she',\n",
    "        'healhtcare': 'healthcare',\n",
    "        'him/her': 'him or her',\n",
    "        'Hindians': 'North Indian who hate British',\n",
    "        'Hinduphobia': 'Hindu phobic',\n",
    "        'hinduphobia': 'Hindu phobic',\n",
    "        'Hinduphobic': 'Hindu phobic',\n",
    "        'hinduphobic': 'Hindu phobic',\n",
    "        'his/her': 'his or her',\n",
    "        'Hongkongese': 'HongKong people',\n",
    "        'hongkongese': 'HongKong people',\n",
    "        'howcan': 'how can',\n",
    "        'Howdo': 'How do',\n",
    "        'howdo': 'how do',\n",
    "        'howdoes': 'how does',\n",
    "        'howmany': 'how many',\n",
    "        'howmuch': 'how much',\n",
    "        'HYPS': ' Harvard Yale Princeton Stanford',\n",
    "        'HYPSM': ' Harvard Yale Princeton Stanford MIT',\n",
    "        'ICOs': 'cryptocurrencies initial coin offering',\n",
    "        'Idiotism': 'idiotism',\n",
    "        'IITian': 'Indian Institutes of Technology student',\n",
    "        'IITians': 'Indian Institutes of Technology students',\n",
    "        'IITJEE': 'Indian Institutes of Technology entrance examination',\n",
    "        ' incel': ' involuntary celibates',\n",
    "        ' incels': ' involuntary celibates',\n",
    "        'indans': 'Indian',\n",
    "        'jallikattu': 'Jallikattu',\n",
    "        'JEE MAINS': 'Indian university entrance examination',\n",
    "        'Jewdar': 'Jew dar',\n",
    "        'Jewism': 'Judaism',\n",
    "        'jewplicate': 'jewish replicate',\n",
    "        'JIIT': 'Jaypee Institute of Information Technology',\n",
    "        'Kalergi': 'Coudenhove-Kalergi',\n",
    "        'Kashmirians': 'Kashmirian',\n",
    "        'Khalistanis': 'Sikh separatist movement',\n",
    "        'Khazari': 'Khazars',\n",
    "        'kompromat': 'compromising material',\n",
    "        'koreaboo': 'Korea boo ',\n",
    "        'KVPY': 'entrance examination',\n",
    "        'labour': 'labor',\n",
    "        'langague': 'language',\n",
    "        'LGBTQ': 'lesbian  gay  bisexual  transgender queer',\n",
    "        'LGBT': 'lesbian  gay  bisexual  transgender',\n",
    "        'Machedo': 'Indian internet celebrity',\n",
    "        'madheshi': 'Madheshi',\n",
    "        'Madridiots': 'Real Madrid idiot supporters',\n",
    "        'mailbait': 'mail bait',\n",
    "        'MAINS': 'exam',\n",
    "        'marathis': 'Marathi',\n",
    "        'marksheet': 'university transcript',\n",
    "        'mastrubate': 'masturbate',\n",
    "        'mastrubating': 'masturbating',\n",
    "        'mastrubation': 'masturbation',\n",
    "        'mastuburate': 'masturbate',\n",
    "        'meninism': 'male feminism',\n",
    "        'MeToo': 'feminist activism campaign',\n",
    "        'Mewani': 'Indian politician Jignesh Mevani',\n",
    "        'MGTOWS': 'Men Going Their Own Way',\n",
    "        'micropenis': 'tiny penis',\n",
    "        'moeslim': 'Muslim',\n",
    "        'mongloid': 'Mongoloid',\n",
    "        'mtech': 'Master of Engineering',\n",
    "        'muhajirs': 'Muslim immigrant',\n",
    "        'Myeshia': 'widow of Green Beret killed in Niger',\n",
    "        'mysoginists': 'misogynists',\n",
    "        'naïve': 'naive',\n",
    "        'narcisist': 'narcissist',\n",
    "        'narcissit': 'narcissist',\n",
    "        'narcissit': 'narcissist',\n",
    "        'Naxali ': 'Naxalite ',\n",
    "        'Naxalities': 'Naxalites',\n",
    "        'NICMAR': 'Indian university',\n",
    "        'Niggeriah': 'Nigger',\n",
    "        'Niggerism': 'Nigger',\n",
    "        'NMAT': 'Indian MBA exam',\n",
    "        'Northindian': 'North Indian ',\n",
    "        'northindian': 'north Indian ',\n",
    "        'northkorea': 'North Korea',\n",
    "        'Novichok': 'Soviet Union agents',\n",
    "        'organisation': 'organization',\n",
    "        'Padmavat': 'Indian Movie Padmaavat',\n",
    "        'Pahul': 'Amrit Sanskar',\n",
    "        'penish': 'penis',\n",
    "        'pennis': 'penis',\n",
    "        'Pizzagate': 'Pizzagate conspiracy theory',\n",
    "        'Pribumi': 'Native Indonesian',\n",
    "        'qouta': 'quota',\n",
    "        'quorans': 'advice website user',\n",
    "        'quoran': 'advice website user',\n",
    "        'Quorans': 'advice website user',\n",
    "        'Quoran': 'advice website user',\n",
    "        'quoras': 'advice website',\n",
    "        'Qoura ': 'advice website ',\n",
    "        'Qoura': 'advice website',\n",
    "        'Quora': 'advice website',\n",
    "        'Quroa': 'advice website',\n",
    "        'QUORA': 'advice website',\n",
    "        'R&D': 'research and development',\n",
    "        'r&d': 'research and development',\n",
    "        'r-aping': 'raping',\n",
    "        'raaping': 'rape',\n",
    "        'rapefugees': 'rapist refugee',\n",
    "        'Rapistan': 'Pakistan rapist',\n",
    "        'rapistan': 'Pakistan rapist',\n",
    "        'Rejuvalex': 'hair growth formula',\n",
    "        'ReleaseTheMemo': 'cry for the right and Trump supporters',\n",
    "        'Remainers': 'anti Brexit',\n",
    "        'remainers': 'anti Brexit',\n",
    "        'remoaner': 'remainer ',\n",
    "        'rohingya': 'Rohingya ',\n",
    "        'sallary': 'salary',\n",
    "        'Sanghis': 'Sanghi',\n",
    "        'sh*t': 'shit',\n",
    "        'shithole': ' shithole ',\n",
    "        'shitlords': 'shit lords',\n",
    "        'shitpost': 'shit post',\n",
    "        'shitslam': 'shit Islam',\n",
    "        'sickular': 'India sick secular ',\n",
    "        'signuficance': 'significance',\n",
    "        'SJW': 'social justice warrior',\n",
    "        'SJWs': 'social justice warrior',\n",
    "        'Skripal': 'Sergei Skripal',\n",
    "        'Strzok': 'Hillary Clinton scandal',\n",
    "        'suckimg': 'sucking',\n",
    "        'superficious': 'superficial',\n",
    "        'Swachh': 'Swachh Bharat mission campaign ',\n",
    "        'Tambrahms': 'Tamil Brahmin',\n",
    "        'Tamilans': 'Tamils',\n",
    "        'Terroristan': 'terrorist Pakistan',\n",
    "        'terroristan': 'terrorist Pakistan',\n",
    "        'Tharki': 'pervert',\n",
    "        'tharki': 'pervert',\n",
    "        'theatre': 'theater',\n",
    "        'theBest': 'the best',\n",
    "        'thighing': 'masturbate',\n",
    "        'travelling': 'traveling',\n",
    "        'trollbots': 'troll bots',\n",
    "        'trollimg': 'trolling',\n",
    "        'trollled': 'trolled',\n",
    "        'Trumpers': 'Trump supporters',\n",
    "        'Trumpanzees': 'Trump chimpanzee fool',\n",
    "        'Turkified': 'Turkification',\n",
    "        'turkified': 'Turkification',\n",
    "        'UCEED': 'Indian Institute of Technology Bombay entrance examination',\n",
    "        'unacadamy': 'Indian online classroom',\n",
    "        'Unacadamy': 'Indian online classroom',\n",
    "        'unoin': 'Union',\n",
    "        'unsincere': 'insincere',\n",
    "        'UPES': 'Indian university',\n",
    "        'UPSEE': 'Indian university entrance examination',\n",
    "        'vaxxer': 'vocal nationalist ',\n",
    "        'VITEEE': 'Vellore institute of technology',\n",
    "        'watsapp': 'Whatsapp',\n",
    "        'whattsapp': 'Whatsapp',\n",
    "        'WBJEE': 'West Bengal entrance examination',\n",
    "        'weatern': 'western',\n",
    "        'westernise': 'westernize',\n",
    "        'Whatare': 'What are',\n",
    "        'whatare': 'what are',\n",
    "        'whst': 'what',\n",
    "        'Whta': 'What',\n",
    "        'whydo': 'why do',\n",
    "        'Whykorean': 'Why Korean',\n",
    "        'Wjy': 'Why',\n",
    "        'WMAF': 'White male married Asian female',\n",
    "        'wumao ': 'cheap Chinese stuff',\n",
    "        'wumaos': 'cheap Chinese stuff',\n",
    "        'wwii': 'world war 2',\n",
    "        ' xender': ' gender',\n",
    "        'XXXTentacion': 'Tentacion',\n",
    "        'youtu ': 'youtube ',\n",
    "        'Zerodha': 'online stock brokerage',\n",
    "        'Žižek': 'Slovenian philosopher Slavoj Žižek',\n",
    "        'Zoë': 'Zoe',\n",
    "        '卐': 'Nazi Germany'\n",
    "    }\n",
    "\n",
    "    escape_cars = re.compile('(\\+|\\*)')\n",
    "    misspell = '|'.join([escape_cars.sub(r\"\\\\\\1\",i) for i in misspell_to_sub.keys()])\n",
    "    misspell_re = re.compile(misspell)\n",
    "    \n",
    "    def _replace(match):\n",
    "        return misspell_to_sub.get(match.group(0), match.group(0))\n",
    "    \n",
    "    return misspell_re.sub(_replace, text)\n",
    "\n",
    "def space_chars(text, chars_to_space):\n",
    "    \"\"\"\n",
    "    Takes a string and list of characters, insert space before and after \n",
    "    characters that appear in text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String to search\n",
    "    chars_to_space : list\n",
    "        list of characters to find and space\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        modified text string    \n",
    "    \"\"\"\n",
    "    \n",
    "    # light arg checking\n",
    "    if type(chars_to_space) == str:\n",
    "        chars_to_space = [chars_to_space]\n",
    "        \n",
    "    chars_to_space = set(chars_to_space)\n",
    "    chars_to_space = '|'.join(chars_to_space)\n",
    "    re_tok = re.compile('({})'.format(chars_to_space))\n",
    "    \n",
    "    return re_tok.sub(r' \\1 ', text)\n",
    "\n",
    "def preprocess(text, remove_num=False):\n",
    "    \n",
    "    # 1. Normalize \n",
    "    # normalize_unicode(text)\n",
    "    \n",
    "    # 2. Remove new-lines\n",
    "    # text = remove_newline(text)\n",
    "    \n",
    "    # 3. replace contractions (e.g. won't -> will not)\n",
    "    text = decontracted(text)\n",
    "\n",
    "    # 4. replace LateX with English\n",
    "    text = clean_latex(text)\n",
    "    \n",
    "    # 5. space characters\n",
    "    text = space_chars(text, ['\\?', ',', '\"', '\\(', '\\)', '%', ':', '\\$', \n",
    "                              '\\.', '\\+', '\\^', '/', '\\{', '\\}', '\\!', \n",
    "                              '#', '=', '-','\\|', '\\[', '\\]','\\.'])\n",
    "    \n",
    "    # 6. handle number\n",
    "    if remove_num:\n",
    "        text = remove_number(text)\n",
    "    else:\n",
    "        text = spacing_digit(text)\n",
    "    \n",
    "    # 7. fix typos and swap terms that are not recognized by embedding\n",
    "    text = clean_misspell(text)\n",
    "\n",
    "    # 8. remove space\n",
    "    text = remove_space(text)\n",
    "    \n",
    "    # 9. remove strings\n",
    "    text = remove_string(text, '_')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.54% of vocabulary words found in embedding files\n",
      "99.48% of corpus found in embedding files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L&amp;T</th>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LNMIIT</th>\n",
       "      <td>72</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kavalireddi</th>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc…</th>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vajiram</th>\n",
       "      <td>61</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngelo</th>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unacademy</th>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJs</th>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Padmaavati</th>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUOET</th>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             frequency  embedding\n",
       "L&T          74         False    \n",
       "LNMIIT       72         False    \n",
       "Kavalireddi  67         False    \n",
       "etc…         65         False    \n",
       "Vajiram      61         False    \n",
       "DAngelo      60         False    \n",
       "Unacademy    60         False    \n",
       "INFJs        58         False    \n",
       "Padmaavati   57         False    \n",
       "MUOET        56         False    "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question_text_pr'] = df_train['question_text'].apply(preprocess)\n",
    "df_test['question_text_pr'] = df_test['question_text'].apply(preprocess)\n",
    "\n",
    "vocab = build_vocab(np.concatenate((df_train.question_text_pr, df_test.question_text_pr)))\n",
    "tokens,c,tokens_in_embedding = vocab_embedding_coverage(vocab, fasttext, True)\n",
    "df_tokens = pd.DataFrame.from_dict(tokens_in_embedding, orient = 'index')\n",
    "df_tokens = df_tokens.sort_values(by='frequency', ascending=False)\n",
    "df_tokens[np.logical_not(df_tokens.embedding)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/processed/preprocessed_train.csv')\n",
    "df_test.to_csv('../data/processed/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Before & After Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_text_pr</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72837</th>\n",
       "      <td>Is the ultimate Republican goal to make the US a sh*tty country ruled by oligarchs propped up by a religious right, with terrible education, a military \"cult\", no privacy rights, no human rights, and no middle class, like the phillipines or turkey?</td>\n",
       "      <td>Is the ultimate Republican goal to make the US a sh*tty country ruled by oligarchs propped up by a religious right , with terrible education , a military \" cult \" , no privacy rights , no human rights , and no middle class , like the phillipines or turkey ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97480</th>\n",
       "      <td>How do you get out of your own head after a really sh*tty day?</td>\n",
       "      <td>How do you get out of your own head after a really sh*tty day ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163578</th>\n",
       "      <td>Why hasn't democracy been digitalized? Are they just slow or is it borderline conspiracy sh*t? Because I feel like voting demographics would drastically change if you could vote via ex. Face ID and interact with your government directly online.</td>\n",
       "      <td>Why has not democracy been digitalized ? Are they just slow or is it borderline conspiracy sh*t ? Because I feel like voting demographics would drastically change if you could vote via ex . Face ID and interact with your government directly online .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212156</th>\n",
       "      <td>Why do some book smells like sh*t?</td>\n",
       "      <td>Why do some book smells like sh*t ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238594</th>\n",
       "      <td>Whydo some creepy messages appear while surfing like - Virus Detected Download this sh*t. Are they serious? Go detail for image.</td>\n",
       "      <td>Whydo some creepy messages appear while surfing like - Virus Detected Download this sh*t . Are they serious ? Go detail for image .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364675</th>\n",
       "      <td>People keep asking \"why do Chinese eat […] \" ten times a day. Chinese master English high enough to debate on Quora, while we can’t read sh*t on their zhihu forums. Am I the only one amazed?</td>\n",
       "      <td>People keep asking \" why do Chinese eat [ … ] \" ten times a day . Chinese master English high enough to debate on advice website , while we can not read sh*t on their zhihu forums . Am I the only one amazed ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                   question_text  \\\n",
       "72837   Is the ultimate Republican goal to make the US a sh*tty country ruled by oligarchs propped up by a religious right, with terrible education, a military \"cult\", no privacy rights, no human rights, and no middle class, like the phillipines or turkey?   \n",
       "97480   How do you get out of your own head after a really sh*tty day?                                                                                                                                                                                             \n",
       "163578  Why hasn't democracy been digitalized? Are they just slow or is it borderline conspiracy sh*t? Because I feel like voting demographics would drastically change if you could vote via ex. Face ID and interact with your government directly online.       \n",
       "212156  Why do some book smells like sh*t?                                                                                                                                                                                                                         \n",
       "238594  Whydo some creepy messages appear while surfing like - Virus Detected Download this sh*t. Are they serious? Go detail for image.                                                                                                                           \n",
       "364675  People keep asking \"why do Chinese eat […] \" ten times a day. Chinese master English high enough to debate on Quora, while we can’t read sh*t on their zhihu forums. Am I the only one amazed?                                                             \n",
       "\n",
       "                                                                                                                                                                                                                                                         question_text_pr  \\\n",
       "72837   Is the ultimate Republican goal to make the US a sh*tty country ruled by oligarchs propped up by a religious right , with terrible education , a military \" cult \" , no privacy rights , no human rights , and no middle class , like the phillipines or turkey ?   \n",
       "97480   How do you get out of your own head after a really sh*tty day ?                                                                                                                                                                                                     \n",
       "163578  Why has not democracy been digitalized ? Are they just slow or is it borderline conspiracy sh*t ? Because I feel like voting demographics would drastically change if you could vote via ex . Face ID and interact with your government directly online .           \n",
       "212156  Why do some book smells like sh*t ?                                                                                                                                                                                                                                 \n",
       "238594  Whydo some creepy messages appear while surfing like - Virus Detected Download this sh*t . Are they serious ? Go detail for image .                                                                                                                                 \n",
       "364675  People keep asking \" why do Chinese eat [ … ] \" ten times a day . Chinese master English high enough to debate on advice website , while we can not read sh*t on their zhihu forums . Am I the only one amazed ?                                                    \n",
       "\n",
       "        target  \n",
       "72837   1       \n",
       "97480   0       \n",
       "163578  1       \n",
       "212156  1       \n",
       "238594  0       \n",
       "364675  1       "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train.question_text.apply(lambda x:\n",
    "                                          'sh*t' in x), ['question_text', 'question_text_pr', 'target']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_text_pr</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>Do you know any Tumblr LGBTQ blogs that accept submissions?</td>\n",
       "      <td>Do you know any Tumblr lesbian gay bisexual transgender queer blogs that accept submissions ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>Researching who is murdering transgender people, a disproportionate number of the murderers are black. Why doesn't the LGBTQI community acknowledge this &amp; why is it that this is happening? Is it due to homophobia &amp; transphobia in the black community?</td>\n",
       "      <td>Researching who is murdering transgender people , a disproportionate number of the murderers are black . Why does not the lesbian gay bisexual transgender queerI community acknowledge this &amp; why is it that this is happening ? Is it due to homophobia &amp; transphobia in the black community ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19625</th>\n",
       "      <td>As the majority of the culture is against the LGBTQ community, can anyone provide proof that this community has committed a crime at any higher rate than society in general?</td>\n",
       "      <td>As the majority of the culture is against the lesbian gay bisexual transgender queer community , can anyone provide proof that this community has committed a crime at any higher rate than society in general ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28182</th>\n",
       "      <td>Where can I read about LGBTQ+ history?</td>\n",
       "      <td>Where can I read about lesbian gay bisexual transgender queer + history ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30531</th>\n",
       "      <td>Is the LGBTQ community to blame for August Ames suicide?</td>\n",
       "      <td>Is the lesbian gay bisexual transgender queer community to blame for August Ames suicide ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34924</th>\n",
       "      <td>Has any political party tried to confine you by your sex, race, age, wealth, gender or LGBTQ ever? How have they?</td>\n",
       "      <td>Has any political party tried to confine you by your sex , race , age , wealth , gender or lesbian gay bisexual transgender queer ever ? How have they ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                    question_text  \\\n",
       "2082   Do you know any Tumblr LGBTQ blogs that accept submissions?                                                                                                                                                                                                  \n",
       "10965  Researching who is murdering transgender people, a disproportionate number of the murderers are black. Why doesn't the LGBTQI community acknowledge this & why is it that this is happening? Is it due to homophobia & transphobia in the black community?   \n",
       "19625  As the majority of the culture is against the LGBTQ community, can anyone provide proof that this community has committed a crime at any higher rate than society in general?                                                                                \n",
       "28182  Where can I read about LGBTQ+ history?                                                                                                                                                                                                                       \n",
       "30531  Is the LGBTQ community to blame for August Ames suicide?                                                                                                                                                                                                     \n",
       "34924  Has any political party tried to confine you by your sex, race, age, wealth, gender or LGBTQ ever? How have they?                                                                                                                                            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                       question_text_pr  \\\n",
       "2082   Do you know any Tumblr lesbian gay bisexual transgender queer blogs that accept submissions ?                                                                                                                                                                                                      \n",
       "10965  Researching who is murdering transgender people , a disproportionate number of the murderers are black . Why does not the lesbian gay bisexual transgender queerI community acknowledge this & why is it that this is happening ? Is it due to homophobia & transphobia in the black community ?   \n",
       "19625  As the majority of the culture is against the lesbian gay bisexual transgender queer community , can anyone provide proof that this community has committed a crime at any higher rate than society in general ?                                                                                   \n",
       "28182  Where can I read about lesbian gay bisexual transgender queer + history ?                                                                                                                                                                                                                          \n",
       "30531  Is the lesbian gay bisexual transgender queer community to blame for August Ames suicide ?                                                                                                                                                                                                         \n",
       "34924  Has any political party tried to confine you by your sex , race , age , wealth , gender or lesbian gay bisexual transgender queer ever ? How have they ?                                                                                                                                           \n",
       "\n",
       "       target  \n",
       "2082   0       \n",
       "10965  1       \n",
       "19625  1       \n",
       "28182  0       \n",
       "30531  1       \n",
       "34924  0       "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train.question_text.apply(lambda x:\n",
    "                                          'LGBTQ' in x), ['question_text', 'question_text_pr', 'target']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L&amp;T</th>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LNMIIT</th>\n",
       "      <td>72</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kavalireddi</th>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc…</th>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vajiram</th>\n",
       "      <td>61</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngelo</th>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unacademy</th>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJs</th>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Padmaavati</th>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUOET</th>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WooCommerce</th>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlShamsi</th>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chsl</th>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modiji</th>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Awdhesh</th>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x²</th>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HackerRank</th>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUBG</th>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIQ</th>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J&amp;K</th>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eLitmus</th>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIBM</th>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S&amp;P</th>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRMJEE</th>\n",
       "      <td>46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHSL</th>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rahu</th>\n",
       "      <td>43</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNIT</th>\n",
       "      <td>43</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGSITS</th>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skripal</th>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What​</th>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuckboy</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P&amp;G</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x³</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncerts</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kotlin</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ehat</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notoo</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinA</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSBI</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blackfyre</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VESIT</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangal</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electroneum</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIBM</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lenskart</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMUCET</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IIPE</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ssc</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can`t</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minance</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ideapad</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*sin</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xam</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPPU</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gurmehar</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bartetzko</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whydoes</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GraphQL</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ifsc</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fooding</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             frequency  embedding\n",
       "L&T          74         False    \n",
       "LNMIIT       72         False    \n",
       "Kavalireddi  67         False    \n",
       "etc…         65         False    \n",
       "Vajiram      61         False    \n",
       "DAngelo      60         False    \n",
       "Unacademy    60         False    \n",
       "INFJs        58         False    \n",
       "Padmaavati   57         False    \n",
       "MUOET        56         False    \n",
       "WooCommerce  56         False    \n",
       "AlShamsi     56         False    \n",
       "chsl         55         False    \n",
       "Modiji       55         False    \n",
       "Awdhesh      54         False    \n",
       "x²           52         False    \n",
       "HackerRank   52         False    \n",
       "PUBG         49         False    \n",
       "AIQ          48         False    \n",
       "J&K          48         False    \n",
       "eLitmus      47         False    \n",
       "SIBM         47         False    \n",
       "S&P          47         False    \n",
       "SRMJEE       46         False    \n",
       "CHSL         45         False    \n",
       "rahu         43         False    \n",
       "MNIT         43         False    \n",
       "SGSITS       42         False    \n",
       "Skripal      42         False    \n",
       "What​        41         False    \n",
       "...          ..           ...    \n",
       "fuckboy      11         False    \n",
       "P&G          11         False    \n",
       "x³           11         False    \n",
       "ncerts       11         False    \n",
       "kotlin       11         False    \n",
       "Ehat         11         False    \n",
       "notoo        11         False    \n",
       "sinA         11         False    \n",
       "MSBI         11         False    \n",
       "Blackfyre    11         False    \n",
       "VESIT        11         False    \n",
       "dangal       11         False    \n",
       "electroneum  11         False    \n",
       "NIBM         11         False    \n",
       "Lenskart     11         False    \n",
       "IMUCET       11         False    \n",
       "IIPE         11         False    \n",
       "Ssc          11         False    \n",
       "can`t        11         False    \n",
       "Minance      11         False    \n",
       "Ideapad      11         False    \n",
       "*sin         11         False    \n",
       "xam          10         False    \n",
       "SPPU         10         False    \n",
       "Gurmehar     10         False    \n",
       "Bartetzko    10         False    \n",
       "Whydoes      10         False    \n",
       "GraphQL      10         False    \n",
       "ifsc         10         False    \n",
       "fooding      10         False    \n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More unmatched items\n",
    "df_tokens[np.logical_not(df_tokens.embedding)].head(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_punctuation(text, other_punct = ['']):\n",
    "    \"\"\"\n",
    "    Does the string contain punctuation?\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String to search for punctuation\n",
    "    other_punct : list\n",
    "        list of additional punctuation characters to search\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if 1 or more instances of punctuation / special characters exist in string\n",
    "    \"\"\"\n",
    "    \n",
    "    # light arg checking\n",
    "    if type(other_punct) == str:\n",
    "        other_punct = [other_punct]\n",
    "    \n",
    "    regular_punct = list(string.punctuation)\n",
    "    extra_punct = [\n",
    "        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "    \n",
    "    all_punct = ''.join(sorted(list(set(regular_punct + extra_punct + other_punct))))\n",
    "    re_tok = re.compile(f'([{all_punct}])')\n",
    "    \n",
    "    return re.search(re_tok, text) is not None\n",
    "\n",
    "# Example\n",
    "# vocab_with_punctuation = [word for word in vocab.keys() if contains_punctuation(word,['∈'])]\n",
    "# random.sample(vocab_with_punctuation,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineered Features for Use w/ Traditional ML\n",
    "\n",
    "Engineered features aren't useful for deep learning models, but can be in traditional ML models, such as SVM or logistic regression applied to TF-IDF. A few potentials were identified in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_words(text):\n",
    "    tokens = text.split()\n",
    "    upper = [1 if u.isupper() and len(u) > 1 else 0 for u in tokens]\n",
    "    return(sum(upper))\n",
    "\n",
    "def programming_related(text):\n",
    "    programming_languages_frameworks_dbs = ['javascript', 'html', 'css', 'sql', 'java', 'bash', 'python',\n",
    "                                        'c#', 'c++', 'c language', 'c programming', 'c programing', 'typescript', 'ruby', 'matlab', 'f#', 'clojure', \n",
    "                                        'haskell', 'erlang', 'coffeescript', 'cobol', 'fortran',\n",
    "                                        'vba', '.net', 'asp.net', 'scala', 'perl', 'php', 'kotlin',\n",
    "                                        'node.js', 'react.js', 'angular', 'django', 'cordova', 'tensorflow', 'keras',\n",
    "                                        'xamarin', 'hadoop', 'pytorch', 'mongo', 'redis', 'elasticsearch', 'mariadb', 'azure',\n",
    "                                        'dynamodb', ' rds', 'redshift', 'cassandra', 'apache hive', 'bigquery', 'hbase',\n",
    "                                        'linux', 'raspberry pi', 'rpi ', 'arduino', 'heroku', 'drupal', \n",
    "                                        'visual studio', 'sublime text', 'rstudio', 'jupyter', 'pycharm', 'netbeans',\n",
    "                                       'emacs', 'vim ', 'komodo', 'graphql', 'golang']\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word.lower() in programming_languages_frameworks_dbs:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "class FeatureEngineering():\n",
    "    def __init__(self, doc_column, max_words=10):\n",
    "        self._most_common = None\n",
    "        self._data = None\n",
    "        self._doc_column = doc_column\n",
    "        self._max_words = max_words\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # save leading tokens information from fit dataset\n",
    "        leading_tokens = df[self._doc_column].apply(lambda x: re.match('\\w+|\\d+|.', x)[0].lower())\n",
    "        leading_token_count = Counter(leading_tokens)\n",
    "        max_count = min(self._max_words, len(leading_token_count)-1)\n",
    "        self._most_common = [w for w,c in leading_token_count.most_common(max_count)]\n",
    "        self._data = df\n",
    "        \n",
    "    def transform(self, df):\n",
    "        # Leading tokens\n",
    "        leading_tokens = df[self._doc_column].apply(lambda x: re.match('\\w+|\\d+|.', x)[0].lower())\n",
    "        df_leading_tokens = leading_tokens.apply(lambda x: x.lower() if x.lower() in self._most_common else 'other')\n",
    "        df_leading_tokens = pd.get_dummies(df_leading_tokens)\n",
    "        for token in self._most_common:\n",
    "            if token not in df_leading_tokens.columns:\n",
    "                df_leading_tokens[token] = 0\n",
    "        df_leading_tokens = df_leading_tokens.rename(columns = {c: 'leading_word_' + c for c in df_leading_tokens.columns})\n",
    "        # Using 'other' as reference category\n",
    "        if 'leading_word_other' in df_leading_tokens.columns:\n",
    "            df_leading_tokens = df_leading_tokens.drop('leading_word_other', axis=1)\n",
    "        df = pd.concat([df, df_leading_tokens], axis=1)\n",
    "        \n",
    "        # Word count\n",
    "        df['word_count'] = df[self._doc_column].apply(lambda x: len(re.findall(r'\\w+',x)))\n",
    "\n",
    "        # Character count\n",
    "        df['char_count'] = df[self._doc_column].apply(lambda x: len(x))\n",
    "\n",
    "        # How many question marks\n",
    "        df['question_mark_count'] = df[self._doc_column].apply(lambda x: len(re.findall(r'\\?',x)))\n",
    "\n",
    "        # LaTex or Math symbols\n",
    "        # Programming questions\n",
    "        df['programming'] = df[self._doc_column].apply(lambda x: programming_related(x))\n",
    "\n",
    "        # ALL CAPS Words\n",
    "        df['caps_count'] = df[self._doc_column].apply(lambda x: count_uppercase_words(x))\n",
    "        \n",
    "        return df\n",
    "\n",
    "ef = FeatureEngineering('question_text', 20)\n",
    "ef.fit(df_train)\n",
    "df_train_plus = ef.transform(df_train)\n",
    "df_test_plus = ef.transform(df_test)\n",
    "\n",
    "df_train.to_csv('../data/processed/preprocessed_features_train.csv')\n",
    "df_test.to_csv('../data/processed/preprocessed_features_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Can basically go on indefinitely doing cleaning and obviously there are diminishing rewards. One can't predict every strange misspelling, minor celebrity's name or creative ways to swear online.\n",
    "\n",
    "Sitting at 66.54% of vocabulary words found in embedding files and 99.48% of corpus found in embedding files is more than a good enough place to work from. I find text cleaning surprisingly satisfying.\n",
    "\n",
    "Next steps:\n",
    "* Revisit topic and sentiment using processed data\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
