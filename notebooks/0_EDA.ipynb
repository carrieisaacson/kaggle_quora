{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Part 1: Exploratory Data Analysis.\n",
    "\n",
    "Digging into the [Quora Insincere Questions Classification](https://www.kaggle.com/c/quora-insincere-questions-classification) training data.\n",
    "\n",
    "From the competition description. Highlights mine:\n",
    "\n",
    "An insincere question is defined as a **question intended to make a statement** rather than look for helpful answers. Some characteristics that can signify that a question is insincere:\n",
    "\n",
    "Has a non-neutral tone\n",
    "Has an exaggerated tone to underscore a point about a group of people\n",
    "Is rhetorical and meant to imply a statement about a group of people\n",
    "Is disparaging or inflammatory\n",
    "Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
    "Makes disparaging attacks/insults against a specific person or group of people\n",
    "Based on an outlandish premise about a group of people\n",
    "Disparages against a characteristic that is not fixable and not measurable\n",
    "Isn't grounded in reality\n",
    "Based on false information, or contains absurd assumptions\n",
    "Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n",
    "The training data includes the question that was asked, and whether it was identified as insincere (target = 1). The ground-truth labels contain some amount of noise: they are not guaranteed to be perfect.\n",
    "\n",
    "Note that the distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and sanitization measures that have been applied to the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select local path vs kaggle kernel\n",
    "path = os.getcwd()\n",
    "if 'data-projects/kaggle_quora/notebooks' in path:\n",
    "    data_dir = '../data/raw/'\n",
    "else:\n",
    "    data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(data_dir +'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "3  000042bf85aa498cd78e   \n",
       "4  0000455dfa3e01eae3af   \n",
       "\n",
       "                                                                       question_text  \\\n",
       "0  How did Quebec nationalists see their province as a nation in the 1960s?            \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2  Why does velocity affect time? Does velocity affect space geometry?                 \n",
       "3  How did Otto von Guericke used the Magdeburg hemispheres?                           \n",
       "4  Can I convert montra helicon D to a mountain bike by just changing the tyres?       \n",
       "\n",
       "   target  \n",
       "0  0       \n",
       "1  0       \n",
       "2  0       \n",
       "3  0       \n",
       "4  0       "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 3)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a feel for insincere vs sincere questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets take a look at the full text for a sample of \"sincere\" questions. They are shorter than I expected. Spelling is fairly good, grammer is a little spottier. Not an overabundance of typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do some Quora users think that short answers are not worthy?\n",
      "What are the implementaion needs to department of aids control?\n",
      "Does every contract need to have an offer and acceptance?\n",
      "Which one has the scope in future an embedded engineer or VLSI engineer?\n",
      "As an Islamic scholar, how would you defend Islam and the Quran from the ill reputation it has garnered from terrorists who base their actions in Islam?\n",
      "What is a twin blaster cannon in Star Wars? What kind of damage can it do?\n",
      "I have around 65% in my engineering. Should I prepare for CAT 2018?\n",
      "What is the easiest way to buy Cardano?\n",
      "Can I watch free pornography?\n",
      "How would a Medieval era society interact with an early Hominid species?\n"
     ]
    }
   ],
   "source": [
    "# Sincere questions\n",
    "for q in dat['question_text'][dat['target'] == 0].sample(10):\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial impressions: \n",
    "Religious / cultural items:\n",
    "\"Why are Muslim beliefs tolerated and accepted, while Nazis beliefs are not, even when they are the same (e.g. belief in superiority over others, hate of homosexuals, disrespect for women)?\"\n",
    "\"If a Hindu woman is not better than a Muslim woman according to Islam, why do Muslims approach Hindu women for love and marriage?\"\n",
    "\"What will you do if you spot a Muslim having sex with a female dog?\"\n",
    "\n",
    "US Political and \"liberal media\" items:\n",
    "\"Is it possible for liberal agents in the FBI to abuse their authority while investigating Trump?\"\n",
    "\"How do the news brainwash people into liberalism (appropriate answers only)?\"\n",
    "\"Why is the Democrats' media suddenly miraculously concerned with the President's ethics and scandals? Not a peep during Obama's eight years.\"\n",
    "\"Is it ok for liberals to insult Trump supporters, but not ok for Trump supporters to insult liberals?\"\n",
    "\"Why do liberals, when they don't have any arguments in political discussion, call the opponent ''racist'', ''sexist'', or ''xenophobic''?\"\n",
    "\"Can Trump supporters read?\"\n",
    "\"When will dictator Donald Trump be tried at the Hague?\"\n",
    "\"Did liberals really believe that Trump's sons illegally shot and killed a Triceratops?\"\n",
    "\n",
    "Indian Political items:\n",
    "\"Does Prakash Raj think that the people of 20 states of India are fools to vote for Modi in assembly elections?\"\n",
    "\n",
    "Sexually \"shocking\" items:\n",
    "\"What does sex with a cousin feels like?\"\n",
    "\"Does anyone of your aunt tries to have sexual relationship with you?\"\n",
    "\n",
    "Transphobic / homophobic items:\n",
    "\"Why do homosexual people exist in nature, it doesn’t make sense biologically speaking?\"\n",
    "\"Does shaving buttocks make you gay?\"\n",
    "\n",
    "Misogynistic / anti-feminist items:\n",
    "\"Do women have emotions and feelings?\"\n",
    "\"Why do feminists think they're for equal rights? When egalitarianism is the real way to achieve this?\"\n",
    "\"Why do some bitches expect virgin guys as partners?\"\n",
    "\n",
    "Racist items:\n",
    "\"Why are blacks so uncivilized?\"\n",
    "\"If we removed all blacks from Obamacare, would it be more affordable to real Americans?\"\n",
    "\n",
    "Nazi / holocaust items:\n",
    "What happens when someone had diarrhea in the holocaust?\n",
    "What is the politically correct spelling: Ass-con-Nazi or Ask-Ken-Nazi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Indian organized crime formed mostly by Muslims and Dalits?\n",
      "What is the legality of incest in Japan?\n",
      "I had a conservative friend ask me how people can be pro-choice and be upset at children being separated from their families at the border, simultaneously. Are conservatives really serious about this?\n",
      "Why are mental health workers fake as fuck?\n",
      "Why do so many Americans, especially high profile one, choose to be willfully ignorant?\n",
      "Could Muslims breed so much that they completely overwhelm non-Muslims and kill us all with an overwhelming weight of numbers, and how can we stop them?\n",
      "Why do Chinese Malaysians believe that Malays will live on trees and in the jungle without Chinese Malaysians developing the country?\n",
      "How can we increase diversity in Israel so Jews will become a minority in a multicultural rainbow nation?\n",
      "[math]24-7=?[/math]\n",
      "Was this site always this mindless or has said situation evolved over time as more people use it?\n"
     ]
    }
   ],
   "source": [
    "# Insincere questions\n",
    "for q in dat['question_text'][dat['target'] == 1].sample(10):\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casual observations:\n",
    "* Text appears longer\n",
    "* More misspellings and gramatical issues, but not as much as I would have expected\n",
    "* More strong language, eg. \"stupid\", \"idiot\", \"hate\", \"irrational\", \"disgusting\"...\n",
    "* More likely to start with \"Why\" rather than \"How\", \"What\" or \"Where\".\n",
    "* What do some \"sincere\" questions about Trump, liberals, Muslims and sex look like? I suspect those are going to be extremely tough to disambiguate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sincere</th>\n",
       "      <th>insincere</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Trump for his money, is it normal?</td>\n",
       "      <td>Are liberals aware that Trump won the popular vote in 30 out of 50 states?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do Donald Trump's first 100 days compare to Obama's first 100?</td>\n",
       "      <td>Why is Trey Gowdy being disloyal to President Trump in Memogate?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does Donald Trump have any good qualities? Are there any standards or lines he won't cross, or does he have any humanizing moments that are sincere?</td>\n",
       "      <td>How severely has Trump damaged the US thus far? How long will it take to recover once he's gone and we can get some less damaging people in the White House and Congress?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is media giving a good news about Trump or bad news about him?</td>\n",
       "      <td>What percentage of the Trump supporters hate Obama merely on the fact that he is black?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How are people waking up from long term coma's reacting to Trump's presidency?</td>\n",
       "      <td>If Donald Trump is so extremely irresponsible, how does he not end up in trouble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Considering how Trump's been doing so far as a president, is it really realistic he'll sit for the entire 4 years?</td>\n",
       "      <td>There are already way too many minorities in the USA. Isn't it about time Trump stopped all refugees and asylum seekers from coming into the USA? The protection was supposed to be 'temporary' anyways?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>By doing the crazy man strategy and everyone else wanting to prove him wrong, is Trump bringing world peace in a bizarre end justifies the mean way?</td>\n",
       "      <td>Why, after Trump cheated to get to be president, should Hillary take his place?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who were the sources for reporting Trump's \"shithole\" comment to the press?</td>\n",
       "      <td>Isn't it time once and for all that Robert Mueller admit there is no connection with Donald Trump and collusion after wasting so much time and taxpayer money?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Is the Trump presidency sustainable?</td>\n",
       "      <td>Why does everyone on Quora hate President Trump?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is Trump the most dishonest president we have ever had?</td>\n",
       "      <td>When will conservative lawmakers and their cohorts finally realize that Trump is a dumpster fire president, as well as a lying, vindictive, shallow con artist with seemingly no sense of decency or mental fortitude?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                sincere  \\\n",
       "0  I love Trump for his money, is it normal?                                                                                                              \n",
       "1  How do Donald Trump's first 100 days compare to Obama's first 100?                                                                                     \n",
       "2  Does Donald Trump have any good qualities? Are there any standards or lines he won't cross, or does he have any humanizing moments that are sincere?   \n",
       "3  Is media giving a good news about Trump or bad news about him?                                                                                         \n",
       "4  How are people waking up from long term coma's reacting to Trump's presidency?                                                                         \n",
       "5  Considering how Trump's been doing so far as a president, is it really realistic he'll sit for the entire 4 years?                                     \n",
       "6  By doing the crazy man strategy and everyone else wanting to prove him wrong, is Trump bringing world peace in a bizarre end justifies the mean way?   \n",
       "7  Who were the sources for reporting Trump's \"shithole\" comment to the press?                                                                            \n",
       "8  Is the Trump presidency sustainable?                                                                                                                   \n",
       "9  Is Trump the most dishonest president we have ever had?                                                                                                \n",
       "\n",
       "                                                                                                                                                                                                                insincere  \n",
       "0  Are liberals aware that Trump won the popular vote in 30 out of 50 states?                                                                                                                                              \n",
       "1  Why is Trey Gowdy being disloyal to President Trump in Memogate?                                                                                                                                                        \n",
       "2  How severely has Trump damaged the US thus far? How long will it take to recover once he's gone and we can get some less damaging people in the White House and Congress?                                               \n",
       "3  What percentage of the Trump supporters hate Obama merely on the fact that he is black?                                                                                                                                 \n",
       "4  If Donald Trump is so extremely irresponsible, how does he not end up in trouble?                                                                                                                                       \n",
       "5  There are already way too many minorities in the USA. Isn't it about time Trump stopped all refugees and asylum seekers from coming into the USA? The protection was supposed to be 'temporary' anyways?                \n",
       "6  Why, after Trump cheated to get to be president, should Hillary take his place?                                                                                                                                         \n",
       "7  Isn't it time once and for all that Robert Mueller admit there is no connection with Donald Trump and collusion after wasting so much time and taxpayer money?                                                          \n",
       "8  Why does everyone on Quora hate President Trump?                                                                                                                                                                        \n",
       "9  When will conservative lawmakers and their cohorts finally realize that Trump is a dumpster fire president, as well as a lying, vindictive, shallow con artist with seemingly no sense of decency or mental fortitude?  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "trump_questions = dat['question_text'].apply(lambda x: \"Trump\" in x)\n",
    "\n",
    "sincere_trump = dat[np.logical_and(trump_questions, dat['target'] == 0)]\n",
    "insincere_trump = dat[np.logical_and(trump_questions, dat['target'] == 1)].sample(10).reset_index()\n",
    "\n",
    "side_by_side_trump = pd.DataFrame({'sincere': sincere_trump['question_text'].sample(10).values,\n",
    "                                                           'insincere': insincere_trump['question_text'].sample(10).values})\n",
    "\n",
    "side_by_side_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were classed as \"sincere\":\n",
    "\n",
    "\"If everyone agrees that Trump is a total failure as president, why is he still president?\"\n",
    "\"Will Trump stage a terrorist attack to gain support for himself and save his administration?\"\n",
    "\"Why are Conservatives so lazy that they have put faith in Trump uplifting their lives?\"\n",
    "\n",
    "And this is \"insincere\":\n",
    "\"Why would President Trump push to move the embassy to Jerusalem? What possible benefit is there to the United States?\"\n",
    "\n",
    "So we can see this is an unevenly applied classification, presumably by human moderators who are doing their darndest. This will be difficult to classify, and I would say that Quora should probably have an iterative model-review-reclassify-remodel process to improve the quality of their tagging, particularly for high-value topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we identify some frequently insincere topics by inspection or other research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US Political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "70.5% insincere - liberal media (43 / 61)\n",
      "63.7% insincere - liberal (2756 / 4324)\n",
      "63.5% insincere - hillary (785 / 1236)\n",
      "61.1% insincere - pizzagate (11 / 18)\n",
      "43.5% insincere - trump (6145 / 14128)\n",
      "23.8% insincere - russia (1374 / 5778)\n",
      "21.4% insincere - spies (27 / 126)\n"
     ]
    }
   ],
   "source": [
    "topics = ['trump', 'hillary', 'liberal', 'russia', 'spies', 'pizzagate', 'liberal media']\n",
    "\n",
    "insincere_count = 0\n",
    "questions_count = 0\n",
    "\n",
    "def calculate_word_target_proportions(df, words, verbose=True):\n",
    "    print('{}% insincere - overall ({} / {})'.format(100*df.target.mean().round(3),\n",
    "                                                    df.target.sum(),\n",
    "                                                    len(df.target)))\n",
    "    props = []\n",
    "    insincere_count = 0\n",
    "    questions_count = 0\n",
    "    for word in words:\n",
    "        filter = df['question_text'].apply(lambda x: word in x.lower())\n",
    "        if filter.sum() > 0:\n",
    "            props.append([(100*df[filter].target.mean()).round(1), \n",
    "                                                            word,\n",
    "                                                           df[filter].target.sum(),\n",
    "                                                           len(df[filter].target)])\n",
    "            insincere_count += df[filter].target.sum()\n",
    "            questions_count += len(df[filter].target)\n",
    "\n",
    "    if verbose:\n",
    "        for prop in sorted(props, reverse=True):\n",
    "            print('{0}% insincere - {1} ({2} / {3})'.format(*prop))\n",
    "        \n",
    "    return props, insincere_count, questions_count\n",
    "        \n",
    "p, i, q = calculate_word_target_proportions(dat, topics)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race, Religion and Cultural Identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "69.5% insincere - black lives (73 / 105)\n",
      "59.0% insincere - muslim (4725 / 8009)\n",
      "37.7% insincere - black (3155 / 8376)\n"
     ]
    }
   ],
   "source": [
    "race_rel_culture = ['black', 'black lives','muslim']\n",
    "p, i, q = calculate_word_target_proportions(dat, race_rel_culture)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender & Sexual Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "51.0% insincere - femin (981 / 1922)\n",
      "43.2% insincere - women (5323 / 12323)\n",
      "42.9% insincere - dyke (3 / 7)\n",
      "42.2% insincere - transgender (341 / 808)\n",
      "35.4% insincere - trans  (90 / 254)\n",
      "30.8% insincere - fag (8 / 26)\n",
      "29.6% insincere -  men (4352 / 14706)\n",
      "22.8% insincere - woman (1194 / 5246)\n",
      "21.6% insincere - queer (16 / 74)\n",
      "11.0% insincere -  man (5317 / 48404)\n"
     ]
    }
   ],
   "source": [
    "gender_sex = ['woman', ' man', 'women', 'trans ', ' men', 'dyke', 'fag', 'queer', 'femin', 'transgender']\n",
    "\n",
    "p, i, q = calculate_word_target_proportions(dat, gender_sex)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex & Anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "77.1% insincere - cunt (54 / 70)\n",
      "69.7% insincere - asshole (168 / 241)\n",
      "61.3% insincere - whore (65 / 106)\n",
      "46.5% insincere - blowjob (53 / 114)\n",
      "45.9% insincere - wank (17 / 37)\n",
      "41.8% insincere - hooker (23 / 55)\n",
      "37.1% insincere - boob (109 / 294)\n",
      "33.3% insincere - buttplug (1 / 3)\n",
      "33.3% insincere - blow job (16 / 48)\n",
      "30.6% insincere - vagina (190 / 621)\n",
      "30.0% insincere - dildo (18 / 60)\n",
      "29.7% insincere - sex (4350 / 14664)\n",
      "27.3% insincere - clitoris (9 / 33)\n",
      "24.2% insincere - cock (149 / 616)\n",
      "13.6% insincere -  ass (1249 / 9165)\n",
      "13.0% insincere - butt (264 / 2026)\n",
      "10.5% insincere - pube (27 / 257)\n",
      "5.8% insincere - twat (7 / 120)\n"
     ]
    }
   ],
   "source": [
    "sex_anatomy = ['sex', 'boob', 'butt', ' ass', 'asshole', 'bollok', 'blow job', 'blowjob', 'bugger'\n",
    "              'boner', 'buttplug', 'dildo', 'clitoris', 'cock', 'cunt', 'twat', 'pube', 'vagina', \n",
    "               'wank', 'whore', 'hooker']\n",
    "\n",
    "p, i, q = calculate_word_target_proportions(dat, sex_anatomy)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words associated with bullying / trolling / insults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taisei Nitta, Fumito Masui, Michal Ptaszynski, Yasutomo Kimura, Rafal Rzepka, and Kenji Araki. 2013. Detecting\n",
    "cyberbullying entries on informal school websites based on category relevance maximization. In IJCNLP,\n",
    "pages 579–586.\n",
    "\n",
    "Raisi, et al (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "69.7% insincere - asshole (168 / 241)\n",
      "65.9% insincere - pussy (149 / 226)\n",
      "65.2% insincere - ignorant (313 / 480)\n",
      "64.9% insincere - idiot (347 / 535)\n",
      "64.3% insincere - nigger (27 / 42)\n",
      "54.5% insincere - dick (331 / 607)\n",
      "53.7% insincere - slut (95 / 177)\n",
      "51.0% insincere - stupid (1066 / 2091)\n",
      "46.2% insincere - fellatio (6 / 13)\n",
      "42.9% insincere - faggot (6 / 14)\n",
      "37.9% insincere - dumb (451 / 1189)\n",
      "37.8% insincere - hate (2670 / 7067)\n",
      "37.6% insincere - disgust (163 / 434)\n",
      "37.1% insincere - nazi (530 / 1427)\n",
      "36.1% insincere - jerk (99 / 274)\n",
      "34.5% insincere - trash (118 / 342)\n",
      "31.8% insincere - prick (14 / 44)\n",
      "31.8% insincere - pathetic (112 / 352)\n",
      "30.8% insincere - fag (8 / 26)\n",
      "30.4% insincere - communist (261 / 858)\n",
      "20.2% insincere - irrational (57 / 282)\n",
      "18.8% insincere - kill (2070 / 11009)\n",
      "18.7% insincere - worthless (32 / 171)\n",
      "18.4% insincere - annoying (115 / 624)\n",
      "9.9% insincere - lying (326 / 3302)\n",
      "9.2% insincere - gross (29 / 315)\n",
      "7.4% insincere - die (1100 / 14900)\n"
     ]
    }
   ],
   "source": [
    "trolling = ['disgust', 'hate', 'idiot', 'stupid', 'irrational', 'annoying', \n",
    "            'gross', 'slut', 'fellatio', 'die', 'kill', 'faggot', 'fag', 'pathetic', 'pussy',\n",
    "           'nigger', 'ignorant', 'lying', 'communist', 'trash', 'dumb', 'worthless',\n",
    "           'nazi', 'jerk', 'asshole', 'dick', 'prick']\n",
    "\n",
    "p, i, q = calculate_word_target_proportions(dat, trolling)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "77.1% insincere - cunt (54 / 70)\n",
      "69.9% insincere - fuck (709 / 1014)\n",
      "69.7% insincere - asshole (168 / 241)\n",
      "59.1% insincere - bitch (146 / 247)\n",
      "54.8% insincere - bastard (51 / 93)\n",
      "50.0% insincere - damn (79 / 158)\n",
      "35.0% insincere - piss (78 / 223)\n",
      "29.7% insincere - poop (107 / 360)\n",
      "21.8% insincere - crap (119 / 547)\n",
      "10.9% insincere - turd (25 / 230)\n",
      "8.2% insincere - ass (3248 / 39664)\n",
      "0.0% insincere - bugger (0 / 6)\n"
     ]
    }
   ],
   "source": [
    "curse = ['asshole', 'fuck', 'cunt', 'ass', 'bastard', 'bitch', 'bugger', 'crap', 'damn', 'f u c k', 'feck'\n",
    "        'hell ', 'piss', 'poop', 'turd']\n",
    "\n",
    "p, i, q = calculate_word_target_proportions(dat, curse)\n",
    "insincere_count = insincere_count + i\n",
    "questions_count = questions_count + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context is pretty important here. Danescu-Niculescu-Mizil's corpus is based on Stack Exchange comments, which are inherently non-poltical. I can see how \"You're welcome\" and \"Thanks a lot\" could be associated with false-sincerity or straight sarcasm on Quora.\n",
    "\n",
    "So I guess, in Quora's case, \"Welcome\" isn't very welcoming! :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: 58935 / Overall Targets: 80810. 72.9%\n",
      "Questions: 244356 / Overall Questions: 1306122. 18.7%\n"
     ]
    }
   ],
   "source": [
    "print('Targets: {} / Overall Targets: {}. {}%'.format(insincere_count, dat['target'].sum(),\n",
    "                                                      (100*insincere_count/dat['target'].sum()).round(1)))\n",
    "print('Questions: {} / Overall Questions: {}. {}%'.format(questions_count, len(dat['target']),\n",
    "                                                          round(100*questions_count/len(dat['target']),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's double counting in here, I'd have to check how much, but these terms account for approximately 19% of questions but 78% of insincere questions.\n",
    "\n",
    "Answer: yes, very much so. References to Liberals and Hillary, the Holocaust and Muslims are flagged as insincere in well over 50% of occurances. References to Trump are insincere over 40% of the time, which is a lot given the base rate is only 6%. So these words along will be strong signals. But there are sincere questions about, say, Trump. An easy model would simply flag all questions about Liberals and Hillary as insincere, and possibly do quite well overall.\n",
    "\n",
    "What will be interesting is the nuance between sincere and popular questions on topics that are frequenly insincere. Consider modelling frequent topics with high target rates separately and ensembling.\n",
    "\n",
    "TODO: Cluster analysis / topic modelling for targets to see if any other themes can be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words associated with politeness\n",
    "\n",
    "Hey, there's some research on identifying *politeness*! This is the opposite, perhaps useful as a feature for non-targets?\n",
    "http://www.cs.cornell.edu/~cristian/Politeness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "21.9% insincere - welcome (71 / 324)\n",
      "13.6% insincere - thanks (59 / 435)\n",
      "12.0% insincere - please (255 / 2124)\n",
      "11.4% insincere - appreciate (61 / 537)\n",
      "7.5% insincere - thank you (21 / 279)\n",
      "6.7% insincere - awesome (22 / 327)\n",
      "4.1% insincere - share (174 / 4277)\n"
     ]
    }
   ],
   "source": [
    "politeness = ['thanks', 'thank you', 'appreciate', 'awesome', 'please', 'share', 'welcome']\n",
    "p, i, q = calculate_word_target_proportions(dat, politeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Internet Abbreviations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2% insincere - overall (80810 / 1306122)\n",
      "100.0% insincere - smh (1 / 1)\n",
      "17.6% insincere - stfu (12 / 68)\n",
      "17.5% insincere - lol (41 / 234)\n",
      "15.6% insincere - omg (5 / 32)\n",
      "9.1% insincere - rolf (1 / 11)\n",
      "4.0% insincere - yolo (1 / 25)\n",
      "1.4% insincere - ikr (1 / 71)\n",
      "0.0% insincere - nvm (0 / 6)\n",
      "0.0% insincere - lmk (0 / 1)\n",
      "0.0% insincere - lmfao (0 / 1)\n"
     ]
    }
   ],
   "source": [
    "internet = ['lol', 'rolf', 'yolo', 'lmfao', 'stfu', 'omg', 'lmk', 'smh', 'nvm', 'ikr']\n",
    "p, i, q = calculate_word_target_proportions(dat, internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very common and not very strong signals. Ok then!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Structure: Leading Why vs How / Which / What?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['leading_word'] = dat['question_text'].apply(lambda x: x.split()[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_targets</th>\n",
       "      <th>percent_of_qs</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading_word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>why</th>\n",
       "      <td>0.225128</td>\n",
       "      <td>10.3</td>\n",
       "      <td>134621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.146523</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.133801</td>\n",
       "      <td>2.7</td>\n",
       "      <td>35523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>0.104519</td>\n",
       "      <td>2.1</td>\n",
       "      <td>27641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.068536</td>\n",
       "      <td>7.3</td>\n",
       "      <td>95191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>0.061599</td>\n",
       "      <td>1.5</td>\n",
       "      <td>20179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.059630</td>\n",
       "      <td>1.6</td>\n",
       "      <td>20560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0.034198</td>\n",
       "      <td>1.6</td>\n",
       "      <td>21054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.029351</td>\n",
       "      <td>3.7</td>\n",
       "      <td>48482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>0.026218</td>\n",
       "      <td>18.9</td>\n",
       "      <td>246468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.018107</td>\n",
       "      <td>1.4</td>\n",
       "      <td>17949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>0.012375</td>\n",
       "      <td>30.3</td>\n",
       "      <td>395226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>0.009870</td>\n",
       "      <td>3.5</td>\n",
       "      <td>45187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              percent_targets  percent_of_qs   count\n",
       "leading_word                                        \n",
       "why           0.225128         10.3           134621\n",
       "are           0.146523         2.0            25839 \n",
       "do            0.133801         2.7            35523 \n",
       "if            0.104519         2.1            27641 \n",
       "is            0.068536         7.3            95191 \n",
       "does          0.061599         1.5            20179 \n",
       "i             0.059630         1.6            20560 \n",
       "who           0.034198         1.6            21054 \n",
       "can           0.029351         3.7            48482 \n",
       "how           0.026218         18.9           246468\n",
       "where         0.018107         1.4            17949 \n",
       "what          0.012375         30.3           395226\n",
       "which         0.009870         3.5            45187 "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = dat.groupby('leading_word')['target'].agg(['mean', 'count']).sort_values(by='mean', ascending=False)\n",
    "summary['percent_of_qs'] = summary['count'].apply(lambda x: round(100*x/dat.shape[0],1))\n",
    "summary = summary.rename(columns={'mean': 'percent_targets'})\n",
    "summary.loc[summary['percent_of_qs'] > 1,['percent_targets', 'percent_of_qs', 'count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading Whys are overwhelmingly far more likely to be insincere! \n",
    "I wonder linguistically what is going on there...\n",
    "\n",
    "Is it that \"why\" and (to a lesser extent) \"are\" lead questions confirmatory phrasing? In that you're trying to get someone to agree with an initial premise. \"Why are Xs into Z?\" or \"Are all Xs into Z\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Quick topic modeling, to see whether anything interesting pops out.\n",
    "Quick and dirty as a first pass, reference [AWS](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) and [DataCamp](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python). \n",
    "(Interestingly there's some verbatim code copying across these two articles. Someone stole someone\n",
    "elses' code and didn't cite them! tut!)\n",
    "\n",
    "TODO: add evaluation of coherence and test different numbers of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    # list for tokenized documents\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    for question in doc_set:\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = question.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "        \n",
    "    return texts\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "# preprocess questions\n",
    "doc_clean = preprocess_data(list(dat.question_text.values))\n",
    "\n",
    "# Create corpus and dictionary\n",
    "dictionary, corpus = prepare_corpus(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(153239 unique tokens: ['1960', 'nation', 'nationalist', 'provinc', 'quebec']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit\n",
    "\n",
    "# # generate LDA model\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "# ldamodel.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.034*\"s\" + 0.013*\"war\" + 0.013*\"class\" + 0.012*\"book\" + 0.012*\"read\" + 0.010*\"major\" + 0.010*\"black\" + 0.010*\"man\" + 0.009*\"c\" + 0.008*\"forc\"'), (1, '0.016*\"state\" + 0.015*\"student\" + 0.013*\"will\" + 0.013*\"s\" + 0.013*\"indian\" + 0.012*\"import\" + 0.011*\"system\" + 0.010*\"scienc\" + 0.010*\"govern\" + 0.010*\"manag\"'), (2, '0.038*\"use\" + 0.035*\"best\" + 0.033*\"can\" + 0.020*\"way\" + 0.018*\"learn\" + 0.018*\"work\" + 0.015*\"compani\" + 0.013*\"start\" + 0.011*\"develop\" + 0.011*\"busi\"'), (3, '0.022*\"engin\" + 0.019*\"trump\" + 0.014*\"mean\" + 0.011*\"prepar\" + 0.011*\"studi\" + 0.011*\"univers\" + 0.011*\"best\" + 0.011*\"cours\" + 0.010*\"social\" + 0.009*\"complet\"'), (4, '0.040*\"can\" + 0.015*\"1\" + 0.015*\"use\" + 0.013*\"buy\" + 0.012*\"best\" + 0.011*\"number\" + 0.011*\"account\" + 0.010*\"servic\" + 0.009*\"get\" + 0.009*\"test\"'), (5, '0.026*\"countri\" + 0.018*\"live\" + 0.017*\"american\" + 0.015*\"peopl\" + 0.014*\"india\" + 0.012*\"like\" + 0.011*\"world\" + 0.010*\"differ\" + 0.010*\"us\" + 0.010*\"consid\"'), (6, '0.031*\"can\" + 0.016*\"quora\" + 0.014*\"question\" + 0.014*\"get\" + 0.013*\"ever\" + 0.012*\"t\" + 0.012*\"s\" + 0.010*\"name\" + 0.010*\"ask\" + 0.009*\"answer\"'), (7, '0.034*\"get\" + 0.034*\"can\" + 0.022*\"job\" + 0.019*\"take\" + 0.017*\"time\" + 0.014*\"school\" + 0.011*\"day\" + 0.011*\"long\" + 0.011*\"colleg\" + 0.011*\"go\"'), (8, '0.035*\"like\" + 0.034*\"t\" + 0.028*\"peopl\" + 0.017*\"feel\" + 0.016*\"can\" + 0.016*\"s\" + 0.015*\"know\" + 0.015*\"don\" + 0.014*\"life\" + 0.014*\"think\"'), (9, '0.032*\"year\" + 0.027*\"can\" + 0.019*\"get\" + 0.015*\"2\" + 0.015*\"will\" + 0.013*\"money\" + 0.012*\"old\" + 0.011*\"3\" + 0.010*\"much\" + 0.010*\"interest\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = ldamodel.get_document_topics(corpus, per_word_topics=False)\n",
    "doc_topics = [doc_topics for doc_topics in corpus_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.35107583), (1, 0.5346326), (2, 0.014286676), (3, 0.014286632), (4, 0.014286356), (5, 0.014287392), (6, 0.014286448), (7, 0.014285714), (8, 0.014286613), (9, 0.014285714)]\n",
      "(1, 0.5346332)\n",
      "Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?\n"
     ]
    }
   ],
   "source": [
    "q_i = 8\n",
    "print(corpus_topics[q_i])\n",
    "print(max(corpus_topics[q_i], key=lambda x: x[1]))\n",
    "print(dat.question_text[q_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is sentiment associated with insincerity?\n",
    "\n",
    "Strictly speaking applying a sentiment analysis involves another corpus, so it isn't valid for this competition. But I'm curious whether my intuition is correct on this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['sentiment'] = dat.question_text.apply(lambda x: TextBlob(x).sentiment)\n",
    "dat['polarity'] = dat['sentiment'].apply(lambda x: x.polarity)\n",
    "dat['subjectivity'] = dat['sentiment'].apply(lambda x: x.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>leading_word</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "      <td>how</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "      <td>do</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "      <td>why</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "      <td>how</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
       "      <td>0</td>\n",
       "      <td>can</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "3  000042bf85aa498cd78e   \n",
       "4  0000455dfa3e01eae3af   \n",
       "\n",
       "                                                                       question_text  \\\n",
       "0  How did Quebec nationalists see their province as a nation in the 1960s?            \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2  Why does velocity affect time? Does velocity affect space geometry?                 \n",
       "3  How did Otto von Guericke used the Magdeburg hemispheres?                           \n",
       "4  Can I convert montra helicon D to a mountain bike by just changing the tyres?       \n",
       "\n",
       "   target leading_word   sentiment  polarity  subjectivity  \n",
       "0  0       how          (0.0, 0.0)  0.0       0.0           \n",
       "1  0       do           (0.0, 0.0)  0.0       0.0           \n",
       "2  0       why          (0.0, 0.0)  0.0       0.0           \n",
       "3  0       how          (0.0, 0.0)  0.0       0.0           \n",
       "4  0       can          (0.0, 0.0)  0.0       0.0           "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104876</td>\n",
       "      <td>0.270181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028165</td>\n",
       "      <td>0.360340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity  subjectivity\n",
       "target                        \n",
       "0       0.104876  0.270181    \n",
       "1       0.028165  0.360340    "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[['polarity', 'subjectivity', 'target']].groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ab00be940>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouped boxplot\n",
    "sns.boxplot(x=\"target\", y=\"polarity\", data=dat, palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1af8507940>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouped boxplot\n",
    "sns.boxplot(x=\"target\", y=\"subjectivity\", data=dat, palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity is slightly higher for the sincere questions, meaning that they are slightly more positive.\n",
    "Subjectivity is slightly higher among insincere questions.\n",
    "These differences aren't large, but they do follow the expected directionality.\n",
    "\n",
    "TextBlob uses the Pattern library, which in turn uses a corpus based on movie reviews. This may not map very well to the types of language used on Quora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
